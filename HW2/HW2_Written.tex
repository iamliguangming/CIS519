\documentclass{article}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{enumitem}

\title{CIS 419/519: Homework 2}
\author{\{Yupeng Li\}}
\date{}

\begin{document}
    \maketitle
    Although the solutions are entirely my own, I consulted with the following people and sources while working on this homework:
     \{http://www.cs.utep.edu/vladik/cs5315.13/cs5315\_13kader.pdf\}
    
    \section{Gradient Descent}
    
    Let k be a counter for the iterations of gradient descent, and let $\alpha_k$ be the learning rate for the $k^{th}$ step of gradient descent.\\
    
    
a.) In one sentence, what are the implications of using a constant value for $\alpha__{k}$ in gradient descent?


The $\alpha_{k}$ used is already the optimum value which will help converge $\theta_{j}$ to the minimum at quick enough speed 


b.) In another sentence, what are the implications for setting $\alpha_{k}$ as a function of k?

We have not yet decided the optimum $\alpha_k$ for convergence, thus we are checking if $\alpha_k$ is too big or too small



    
    
    \section{Linear Regression}


        
        
\end{document}