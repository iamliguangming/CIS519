{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIO3UIZe6wsZ"
   },
   "source": [
    "# CIS 419/519 \n",
    "#**Homework 3 : Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gS022EH9_-p"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjPfIJ5G52It"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UodjntNc6Ex2"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, alpha = 0.01, regLambda=0.01, regNorm=2, epsilon=0.0001, maxNumIters = 10000, initTheta = None):\n",
    "        '''\n",
    "        Constructor\n",
    "        Arguments:\n",
    "        \talpha is the learning rate\n",
    "        \tregLambda is the regularization parameter\n",
    "        \tregNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
    "        \tepsilon is the convergence parameter\n",
    "        \tmaxNumIters is the maximum number of iterations to run\n",
    "          initTheta is the initial theta value. This is an optional argument\n",
    "        '''\n",
    "\n",
    "    \n",
    "\n",
    "    def computeCost(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-by-1 numpy matrix\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "        '''\n",
    "\n",
    "    \n",
    "    \n",
    "    def computeGradient(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the gradient of the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-by-1 numpy matrix\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            the gradient, an d-dimensional vector\n",
    "        '''\n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "            y is an n-by-1 Pandas data frame\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before fit() is called.\n",
    "        '''\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "        Returns:\n",
    "            an n-by-1 dimensional Pandas data frame of the predictions\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before predict() is called.\n",
    "        '''\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Used the model to predict the class probability for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "        Returns:\n",
    "            an n-by-1 Pandas data frame of the class probabilities\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before predict_proba() is called.\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "    \t'''\n",
    "    \tComputes the sigmoid function 1/(1+exp(-z))\n",
    "    \t'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Y-_IFEK6g4Q"
   },
   "source": [
    "# Test Logistic Regression 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7Un3fMJ6keB"
   },
   "outputs": [],
   "source": [
    "# Test script for training a logistic regressiom model\n",
    "#\n",
    "# This code should run successfully without changes if your implementation is correct\n",
    "#\n",
    "from numpy import loadtxt, ones, zeros, where\n",
    "import numpy as np\n",
    "from pylab import plot,legend,show,where,scatter,xlabel, ylabel,linspace,contour,title\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_logreg1():\n",
    "    # load the data\n",
    "    filepath = \"http://www.seas.upenn.edu/~cis519/spring2020/data/hw3-data1.csv\"\n",
    "    df = pd.read_csv(filepath, header=None)\n",
    "\n",
    "    X = df[df.columns[0:2]]\n",
    "    y = df[df.columns[2]]\n",
    "\n",
    "    n,d = X.shape\n",
    "\n",
    "    # # Standardize features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    standardizer = StandardScaler()\n",
    "    Xstandardized = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
    "    \n",
    "    # train logistic regression\n",
    "    logregModel = LogisticRegression(regLambda = 0.00000001)\n",
    "    logregModel.fit(Xstandardized,y)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min = X[X.columns[0]].min() - .5\n",
    "    x_max = X[X.columns[0]].max() + .5\n",
    "    y_min = X[X.columns[1]].min() - .5\n",
    "    y_max = X[X.columns[1]].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    allPoints = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
    "    allPoints = pd.DataFrame(standardizer.transform(allPoints))\n",
    "    Z = logregModel.predict(allPoints)\n",
    "    Z = np.asmatrix(Z.to_numpy())\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Plot the training points\n",
    "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c=y.ravel(), edgecolors='k', cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Configure the plot display\n",
    "    plt.xlabel('Exam 1 Score')\n",
    "    plt.ylabel('Exam 2 Score')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "test_logreg1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9HL_cUX6NYm"
   },
   "source": [
    "# Map Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7uldP0A6Hcn"
   },
   "outputs": [],
   "source": [
    "def mapFeature(X, column1, column2, maxPower = 6):\n",
    "    '''\n",
    "    Maps the two specified input features to quadratic features. Does not standardize any features.\n",
    "        \n",
    "    Returns a new feature array with d features, comprising of\n",
    "        X1, X2, X1 ** 2, X2 ** 2, X1*X2, X1*X2 ** 2, ... up to the maxPower polynomial\n",
    "        \n",
    "    Arguments:\n",
    "        X is an n-by-d Pandas data frame, where d > 2\n",
    "        column1 is the string specifying the column name corresponding to feature X1\n",
    "        column2 is the string specifying the column name corresponding to feature X2\n",
    "    Returns:\n",
    "        an n-by-d2 Pandas data frame, where each row represents the original features augmented with the new features of the corresponding instance\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcBEI53O6lde"
   },
   "source": [
    "# Test Logistic Regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGqdTWMU6oDH"
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt, ones, zeros, where\n",
    "import numpy as np\n",
    "from pylab import plot,legend,show,where,scatter,xlabel, ylabel,linspace,contour,title\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_logreg2():\n",
    "\n",
    "    polyPower = 6\n",
    "\n",
    "    # load the data\n",
    "    filepath = \"http://www.seas.upenn.edu/~cis519/spring2020/data/hw3-data2.csv\"\n",
    "    df = pd.read_csv(filepath, header=None)\n",
    "\n",
    "    X = df[df.columns[0:2]]\n",
    "    y = df[df.columns[2]]\n",
    "\n",
    "    n,d = X.shape\n",
    "\n",
    "    # map features into a higher dimensional feature space\n",
    "    Xaug = mapFeature(X.copy(), X.columns[0], X.columns[1], polyPower)\n",
    "\n",
    "    # # Standardize features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    standardizer = StandardScaler()\n",
    "    Xaug = pd.DataFrame(standardizer.fit_transform(Xaug))  # compute mean and stdev on training set for standardization\n",
    "    \n",
    "    # train logistic regression\n",
    "    logregModel = LogisticRegression(regLambda = 0.00000001, regNorm=2)\n",
    "    logregModel.fit(Xaug,y)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min = X[X.columns[0]].min() - .5\n",
    "    x_max = X[X.columns[0]].max() + .5\n",
    "    y_min = X[X.columns[1]].min() - .5\n",
    "    y_max = X[X.columns[1]].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    allPoints = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
    "    allPoints = mapFeature(allPoints, allPoints.columns[0], allPoints.columns[1], polyPower)\n",
    "    allPoints = pd.DataFrame(standardizer.transform(allPoints))\n",
    "    Xaug = pd.DataFrame(standardizer.fit_transform(Xaug))  # standardize data\n",
    "    \n",
    "    Z = logregModel.predict(allPoints)\n",
    "    Z = np.asmatrix(Z.to_numpy())\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Plot the training points\n",
    "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c=y.ravel(), edgecolors='k', cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Configure the plot display\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(str(Z.min()) + \" \" + str(Z.max()))\n",
    "\n",
    "test_logreg2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7ef6eUW7BNy"
   },
   "source": [
    "# Logistic Regression with Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zcisRww7Y3X"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionAdagrad:\n",
    "\n",
    "    def __init__(self, alpha = 0.01, regLambda=0.01, regNorm=2, epsilon=0.0001, maxNumIters = 10000, initTheta = None):\n",
    "        '''\n",
    "        Constructor\n",
    "        Arguments:\n",
    "        \talpha is the learning rate\n",
    "        \tregLambda is the regularization parameter\n",
    "        \tregNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
    "        \tepsilon is the convergence parameter\n",
    "        \tmaxNumIters is the maximum number of iterations to run\n",
    "          initTheta is the initial theta value. This is an optional argument\n",
    "        '''\n",
    "\n",
    "    \n",
    "\n",
    "    def computeCost(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-by-1 numpy matrix\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "        '''\n",
    "\n",
    "    \n",
    "    \n",
    "    def computeGradient(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the gradient of the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-by-1 numpy matrix\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            the gradient, an d-dimensional vector\n",
    "        '''\n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "            y is an n-by-1 Pandas data frame\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before fit() is called.\n",
    "        '''\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "        Returns:\n",
    "            an n-by-1 dimensional Pandas data frame of the predictions\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before predict() is called.\n",
    "        '''\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Used the model to predict the class probability for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "        Returns:\n",
    "            an n-by-1 Pandas data frame of the class probabilities\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before predict_proba() is called.\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "    \t'''\n",
    "    \tComputes the sigmoid function 1/(1+exp(-z))\n",
    "    \t'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
